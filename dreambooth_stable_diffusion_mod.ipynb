{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/thx-pw/37ba770dfd2b15f119ec8032c3ae90a1/dreambooth_stable_diffusion_mod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XU7NuMAA2drw"
      },
      "outputs": [],
      "source": [
        "#@markdown Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM7j0ZSc_9c"
      },
      "source": [
        "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTMyW41cC1E"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWXPZqjsZVV"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.12.0 transformers ftfy bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4lqqWT_uxD2"
      },
      "outputs": [],
      "source": [
        "#@title Login to HuggingFace ðŸ¤—\n",
        "\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree. You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "from huggingface_hub import notebook_login\n",
        "!git config --global credential.helper store\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfTlc8Mqb8iH"
      },
      "source": [
        "### Install xformers from precompiled wheel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6dcjPnnaiCn"
      },
      "outputs": [],
      "source": [
        "%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac_various_6/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4, should also work on P100, thanks to https://github.com/metrolobo\n",
        "\n",
        "# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@1d31a3a#egg=xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NV324ZcL9L"
      },
      "source": [
        "## Settings and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"Nilaier/Waifu-Diffusers\" #@param [\"hakurei/waifu-diffusion\", \"Nilaier/Waifu-Diffusers\",\"CompVis/stable-diffusion-v1-4\", \"naclbit/trinart_stable_diffusion_v2,diffusers-115k\", \"naclbit/trinart_stable_diffusion_v2,diffusers-95k\", \"naclbit/trinart_stable_diffusion_v2,diffusers-60k\"] {allow-input: true}\n",
        "\n",
        "#@markdown Path for images of the concept for training.\n",
        "INSTANCE_DIR = \"/content/data/sks\" #@param {type:\"string\"}\n",
        "!mkdir -p $INSTANCE_DIR\n",
        "\n",
        "#@markdown A general name for class like dog for dog images.\n",
        "CLASS_NAME = \"1boy\" #@param {type:\"string\"}\n",
        "CLASS_DIR = f\"/content/data/{CLASS_NAME}\"\n",
        "\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/sks\" #@param {type:\"string\"}\n",
        "OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "#@markdown sks is a rare identifier, feel free to replace it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fe-GgtnUVO_e"
      },
      "outputs": [],
      "source": [
        "#@markdown Upload your images by running this cell.\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to INSTANCE_DIR (it uploads faster)\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    dst_path = os.path.join(INSTANCE_DIR, filename)\n",
        "    shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create train_dreambooth_mod.py\n",
        "%%writefile train_dreambooth_mod.py\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import set_seed\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from huggingface_hub import HfFolder, Repository, whoami\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
        "    parser.add_argument(\n",
        "        \"--pretrained_model_name_or_path\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        required=True,\n",
        "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--instance_data_dir\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        required=True,\n",
        "        help=\"A folder containing the training data of instance images.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--class_data_dir\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        required=False,\n",
        "        help=\"A folder containing the training data of class images.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--instance_prompt\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"The prompt with identifier specifying the instance\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--class_prompt\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"The prompt to specify images in the same class as provided instance images.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--with_prior_preservation\",\n",
        "        default=False,\n",
        "        action=\"store_true\",\n",
        "        help=\"Flag to add prior preservation loss.\",\n",
        "    )\n",
        "    parser.add_argument(\"--prior_loss_weight\", type=float, default=1.0, help=\"The weight of prior preservation loss.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_class_images\",\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help=(\n",
        "            \"Minimal class images for prior perversation loss. If not have enough images, additional images will be\"\n",
        "            \" sampled with class_prompt.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        type=str,\n",
        "        default=\"text-inversion-model\",\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n",
        "    parser.add_argument(\n",
        "        \"--resolution\",\n",
        "        type=int,\n",
        "        default=512,\n",
        "        help=(\n",
        "            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n",
        "            \" resolution\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--center_crop\", action=\"store_true\", help=\"Whether to center crop images before resizing to resolution\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_batch_size\", type=int, default=4, help=\"Batch size (per device) for the training dataloader.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--sample_batch_size\", type=int, default=4, help=\"Batch size (per device) for sampling images.\"\n",
        "    )\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n",
        "    parser.add_argument(\n",
        "        \"--max_train_steps\",\n",
        "        type=int,\n",
        "        default=None,\n",
        "        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_checkpointing\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning_rate\",\n",
        "        type=float,\n",
        "        default=5e-6,\n",
        "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--scale_lr\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr_scheduler\",\n",
        "        type=str,\n",
        "        default=\"constant\",\n",
        "        help=(\n",
        "            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n",
        "            ' \"constant\", \"constant_with_warmup\"]'\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n",
        "    )\n",
        "    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n",
        "    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n",
        "    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n",
        "    parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n",
        "    parser.add_argument(\n",
        "        \"--hub_model_id\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--logging_dir\",\n",
        "        type=str,\n",
        "        default=\"logs\",\n",
        "        help=(\n",
        "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
        "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\"--log_interval\", type=int, default=10, help=\"Log every N steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--mixed_precision\",\n",
        "        type=str,\n",
        "        default=\"no\",\n",
        "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
        "        help=(\n",
        "            \"Whether to use mixed precision. Choose\"\n",
        "            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
        "            \"and an Nvidia Ampere GPU.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\"--not_cache_latents\", action=\"store_true\", help=\"Do not precompute and cache latents from VAE.\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
        "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
        "        args.local_rank = env_local_rank\n",
        "\n",
        "    if args.instance_data_dir is None:\n",
        "        raise ValueError(\"You must specify a train data directory.\")\n",
        "\n",
        "    if args.with_prior_preservation:\n",
        "        if args.class_data_dir is None:\n",
        "            raise ValueError(\"You must specify a data directory for class images.\")\n",
        "        if args.class_prompt is None:\n",
        "            raise ValueError(\"You must specify prompt for class images.\")\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
        "    It pre-processes the images and the tokenizes prompts.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        instance_data_root,\n",
        "        instance_prompt,\n",
        "        tokenizer,\n",
        "        class_data_root=None,\n",
        "        class_prompt=None,\n",
        "        size=512,\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.instance_data_root = Path(instance_data_root)\n",
        "        if not self.instance_data_root.exists():\n",
        "            raise ValueError(\"Instance images root doesn't exists.\")\n",
        "\n",
        "        self.instance_images_path = [x for x in Path(instance_data_root).iterdir() if x.is_file()]\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.instance_prompt = instance_prompt\n",
        "        self._length = self.num_instance_images\n",
        "\n",
        "        if class_data_root is not None:\n",
        "            self.class_data_root = Path(class_data_root)\n",
        "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
        "            self.class_images_path = [x for x in self.class_data_root.iterdir() if x.is_file()]\n",
        "            self.num_class_images = len(self.class_images_path)\n",
        "            self._length = max(self.num_class_images, self.num_instance_images)\n",
        "            self.class_prompt = class_prompt\n",
        "        else:\n",
        "            self.class_data_root = None\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "        if not instance_image.mode == \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.instance_prompt,\n",
        "            padding=\"do_not_pad\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        ).input_ids\n",
        "\n",
        "        if self.class_data_root:\n",
        "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
        "            if not class_image.mode == \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                self.class_prompt,\n",
        "                padding=\"do_not_pad\",\n",
        "                truncation=True,\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "            ).input_ids\n",
        "\n",
        "        return example\n",
        "\n",
        "\n",
        "class PromptDataset(Dataset):\n",
        "    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n",
        "\n",
        "    def __init__(self, prompt, num_samples):\n",
        "        self.prompt = prompt\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        example[\"prompt\"] = self.prompt\n",
        "        example[\"index\"] = index\n",
        "        return example\n",
        "\n",
        "\n",
        "class LatentsDataset(Dataset):\n",
        "    def __init__(self, latents_cache, texts):\n",
        "        self.latents_cache = latents_cache\n",
        "        self.texts = texts\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.latents_cache)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.latents_cache[index], self.texts[index]\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self, name=None):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = self.count = self.avg = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n",
        "    if token is None:\n",
        "        token = HfFolder.get_token()\n",
        "    if organization is None:\n",
        "        username = whoami(token)[\"name\"]\n",
        "        return f\"{username}/{model_id}\"\n",
        "    else:\n",
        "        return f\"{organization}/{model_id}\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    logging_dir = Path(args.output_dir, args.logging_dir)\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        mixed_precision=args.mixed_precision,\n",
        "        log_with=\"tensorboard\",\n",
        "        logging_dir=logging_dir,\n",
        "    )\n",
        "\n",
        "    if args.seed is not None:\n",
        "        set_seed(args.seed)\n",
        "\n",
        "    if args.with_prior_preservation:\n",
        "        class_images_dir = Path(args.class_data_dir)\n",
        "        if not class_images_dir.exists():\n",
        "            class_images_dir.mkdir(parents=True)\n",
        "        cur_class_images = len(list(class_images_dir.iterdir()))\n",
        "\n",
        "        if cur_class_images < args.num_class_images:\n",
        "            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n",
        "            pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                args.pretrained_model_name_or_path, torch_dtype=torch_dtype, use_auth_token=True\n",
        "            )\n",
        "            pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "            num_new_images = args.num_class_images - cur_class_images\n",
        "            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n",
        "\n",
        "            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n",
        "            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n",
        "\n",
        "            sample_dataloader = accelerator.prepare(sample_dataloader)\n",
        "            pipeline.to(accelerator.device)\n",
        "\n",
        "            with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "                for example in tqdm(\n",
        "                    sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n",
        "                ):\n",
        "                    images = pipeline(example[\"prompt\"]).images\n",
        "\n",
        "                    for i, image in enumerate(images):\n",
        "                        image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n",
        "\n",
        "            del pipeline\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if accelerator.is_main_process:\n",
        "        if args.push_to_hub:\n",
        "            if args.hub_model_id is None:\n",
        "                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
        "            else:\n",
        "                repo_name = args.hub_model_id\n",
        "            repo = Repository(args.output_dir, clone_from=repo_name)\n",
        "\n",
        "            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "                if \"step_*\" not in gitignore:\n",
        "                    gitignore.write(\"step_*\\n\")\n",
        "                if \"epoch_*\" not in gitignore:\n",
        "                    gitignore.write(\"epoch_*\\n\")\n",
        "        elif args.output_dir is not None:\n",
        "            os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    # Load the tokenizer\n",
        "    if args.tokenizer_name:\n",
        "        tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)\n",
        "    elif args.pretrained_model_name_or_path:\n",
        "        tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\", use_auth_token=True)\n",
        "\n",
        "    # Load models and create wrapper for stable diffusion\n",
        "    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\", use_auth_token=True)\n",
        "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", use_auth_token=True)\n",
        "    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\", use_auth_token=True)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "\n",
        "    if args.scale_lr:\n",
        "        args.learning_rate = (\n",
        "            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
        "        )\n",
        "\n",
        "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
        "    if args.use_8bit_adam:\n",
        "        try:\n",
        "            import bitsandbytes as bnb\n",
        "        except ImportError:\n",
        "            raise ImportError(\n",
        "                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
        "            )\n",
        "        print(\"Use AdamW8bit optimizer\")\n",
        "        optimizer_class = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_class = torch.optim.AdamW\n",
        "\n",
        "    # create wrapper model\n",
        "    import torch.nn as nn\n",
        "    class WrapperModel(nn.Module):\n",
        "        def __init__(self, un, te):\n",
        "            super().__init__()\n",
        "            self.unet = un\n",
        "            self.text_encoder = te\n",
        "    \n",
        "    model = WrapperModel(unet, text_encoder)\n",
        "\n",
        "    optimizer = optimizer_class(\n",
        "        # unet.parameters(),  # only optimize unet\n",
        "        model.parameters(), \n",
        "        lr=args.learning_rate,\n",
        "        betas=(args.adam_beta1, args.adam_beta2),\n",
        "        weight_decay=args.adam_weight_decay,\n",
        "        eps=args.adam_epsilon,\n",
        "    )\n",
        "\n",
        "    noise_scheduler = DDPMScheduler(\n",
        "        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n",
        "    )\n",
        "\n",
        "    train_dataset = DreamBoothDataset(\n",
        "        instance_data_root=args.instance_data_dir,\n",
        "        instance_prompt=args.instance_prompt,\n",
        "        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
        "        class_prompt=args.class_prompt,\n",
        "        tokenizer=tokenizer,\n",
        "        size=args.resolution,\n",
        "        center_crop=args.center_crop,\n",
        "    )\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "\n",
        "        # Concat class and instance examples for prior preservation.\n",
        "        # We do this to avoid doing two forward passes.\n",
        "        if args.with_prior_preservation:\n",
        "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
        "            pixel_values += [example[\"class_images\"] for example in examples]\n",
        "\n",
        "        pixel_values = torch.stack(pixel_values)\n",
        "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "\n",
        "        input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"pixel_values\": pixel_values,\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Move vae to gpu\n",
        "    vae.to(accelerator.device)\n",
        "\n",
        "    if not args.not_cache_latents:\n",
        "        latents_cache = []\n",
        "        texts = []\n",
        "        for batch in tqdm(train_dataloader, desc=\"Caching latents\"):\n",
        "            with torch.no_grad():\n",
        "                batch[\"pixel_values\"] = batch[\"pixel_values\"].to(accelerator.device, non_blocking=True)\n",
        "                latents_cache.append(vae.encode(batch[\"pixel_values\"]).latent_dist)\n",
        "                texts.append(batch[\"input_ids\"])\n",
        "        train_dataset = LatentsDataset(latents_cache, texts)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, collate_fn=lambda x: x, shuffle=True)\n",
        "\n",
        "        del vae\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Scheduler and math around the number of training steps.\n",
        "    overrode_max_train_steps = False\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if args.max_train_steps is None:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "        overrode_max_train_steps = True\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        args.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
        "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
        "    )\n",
        "\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if overrode_max_train_steps:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    # Afterwards we recalculate our number of training epochs\n",
        "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    # We need to initialize the trackers we use, and also store our configuration.\n",
        "    # The trackers initializes automatically on the main process.\n",
        "    if accelerator.is_main_process:\n",
        "        accelerator.init_trackers(\"dreambooth\", config=vars(args))\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
        "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "    # Only show the progress bar once on each machine.\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(\"Steps\")\n",
        "    global_step = 0\n",
        "    loss_avg = AverageMeter()\n",
        "    for epoch in range(args.num_train_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(model):\n",
        "                # Convert images to latent space\n",
        "                with torch.no_grad():\n",
        "                    if not args.not_cache_latents:\n",
        "                        latent_dist = batch[0][0]\n",
        "                    else:\n",
        "                        latent_dist = vae.encode(batch[\"pixel_values\"]).latent_dist\n",
        "                    latents = latent_dist.sample() * 0.18215\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn(latents.shape).to(latents.device)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "                timesteps = timesteps.long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                #     with torch.no_grad():\n",
        "                #         if not args.not_cache_latents:\n",
        "                #             encoder_hidden_states = batch[0][1]\n",
        "                #         else:\n",
        "                #             encoder_hidden_states = text_encoder(batch[0][1])[0]\n",
        "                # with gradient\n",
        "                encoder_hidden_states = text_encoder(batch[0][1])[0]\n",
        "\n",
        "                # Predict the noise residual\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                if args.with_prior_preservation:\n",
        "                    # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n",
        "                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
        "                    noise, noise_prior = torch.chunk(noise, 2, dim=0)\n",
        "\n",
        "                    # Compute instance loss\n",
        "                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                    # Compute prior loss\n",
        "                    prior_loss = F.mse_loss(noise_pred_prior, noise_prior, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                    # Add the prior loss to the instance loss.\n",
        "                    loss = loss + args.prior_loss_weight * prior_loss\n",
        "                else:\n",
        "                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss_avg.update(loss.detach_(), bsz)\n",
        "\n",
        "            if not global_step % args.log_interval:\n",
        "                logs = {\"loss\": loss_avg.avg.item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "                progress_bar.set_postfix(**logs)\n",
        "                accelerator.log(logs, step=global_step)\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "        accelerator.wait_for_everyone()\n",
        "\n",
        "    # Create the pipeline using using the trained modules and save it.\n",
        "    if accelerator.is_main_process:\n",
        "        unwrapped = accelerator.unwrap_model(model)\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            args.pretrained_model_name_or_path,\n",
        "            text_encoder=unwrapped.text_encoder, \n",
        "            unet=unwrapped.unet,\n",
        "            use_auth_token=True,\n",
        "        )\n",
        "        pipeline.save_pretrained(args.output_dir)\n",
        "\n",
        "        if args.push_to_hub:\n",
        "            repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\n",
        "\n",
        "    accelerator.end_training()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HQvQeZSbFpf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg"
      },
      "outputs": [],
      "source": [
        "!accelerate launch --num_cpu_threads_per_process 8 train_dreambooth_mod.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --instance_data_dir=$INSTANCE_DIR \\\n",
        "  --class_data_dir=$CLASS_DIR \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --instance_prompt=\"sks {CLASS_NAME}\" \\\n",
        "  --class_prompt=\"{CLASS_NAME}\" \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=2 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --use_8bit_adam \\\n",
        "  --learning_rate=2e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=24 \\\n",
        "  --sample_batch_size=2 \\\n",
        "  --mixed_precision=\"no\" \\\n",
        "  --max_train_steps=400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8wgU0HN-Kq"
      },
      "source": [
        "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "baL22PHzOLeP"
      },
      "outputs": [],
      "source": [
        "#@markdown Download script\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "89Az5NUxOWdy"
      },
      "outputs": [],
      "source": [
        "#@markdown Run conversion.\n",
        "ckpt_path = OUTPUT_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB), might loose some quality.\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $OUTPUT_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save ckpt to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp {ckpt_path} /content/drive/MyDrive/"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MYDjfXf8MB2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG4fd_dTbF"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = OUTPUT_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oIzkltjpVO_f"
      },
      "outputs": [],
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6xoHWSsbcS3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run for generating images.\n",
        "\n",
        "prompt = \"a sks 1boy in MacDonald's\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 50 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe([prompt] * num_samples, height=height, width=width, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, generator=g_cuda).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WMCqQ5Tcdsm2"
      },
      "outputs": [],
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "            [prompt]*int(num_samples), height=int(height), width=int(width),\n",
        "            num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "            generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of sks guy, digital painting\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XfTlc8Mqb8iH"
      ],
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}